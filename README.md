# python-project
I will update some pet projects coded with python, suitable for practice.
上传的第一份文件的主题是CNN中文文本分类。
文件中没有附加文本获取的网络爬虫代码，分词、停用关键词等的预处理代码，在word_div文件夹中添加了处理完后的五类文本共10024条文本可以直接用于训练。
train.py中包含了训练所用的代码，利用keras搭建简单的CNN网络，利用配置好的GPU环境进行训练，训练前从总样本中分出了30%的样本作为测试集，训练时从训练集中再分出30%作为验证集；网络中添加Ebedding将词表示为100维的向量，三层卷积加上池化（具体见train.py）。
训练结果方面，共训练100个epoch，但是大概在50epoch后出现过拟合的情况，验证集的精度停留在大约70%的水平（见100epochs.png），可以利用50epoch训练出的模型进行预测，具备一定的泛化能力，在测试集上的表现为69%。
整体看，模型的精度差强人意。首先，训练集的数据量是一个问题，10024数据量的小文本量使用向量机的效果可能会更好；其次，网络架构可以调整的。
不过这方面没有什么实践经验，小伙伴们看了以后如果有什么好的建议可以私我：hahzmjm@163.com。
（未完待续）
